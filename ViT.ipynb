{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaimil.d/miniconda3/envs/econ424/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/jaimil.d/miniconda3/envs/econ424/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/jaimil.d/miniconda3/envs/econ424/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <2A8DB508-8AAF-3FF1-BDFE-9EF17CC2B482> /Users/jaimil.d/miniconda3/envs/econ424/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class CFG:\n",
    "    seed = 480  # Random seed\n",
    "    batch_size = 32  # Batch size\n",
    "    num_classes = 6  # Number of classes in the dataset\n",
    "    image_size = [224, 224]  # Input image size\n",
    "    epochs = 10  # Training epochs\n",
    "    class_names = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']\n",
    "    num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15afb1210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(CFG.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ./data/train_images/101801795.jpeg\n",
       "1        ./data/train_images/115813315.jpeg\n",
       "2        ./data/train_images/173551949.jpeg\n",
       "3        ./data/train_images/148811120.jpeg\n",
       "4        ./data/train_images/195108876.jpeg\n",
       "                        ...                \n",
       "43358    ./data/train_images/172502909.jpeg\n",
       "43359    ./data/train_images/183294324.jpeg\n",
       "43360    ./data/train_images/108577580.jpeg\n",
       "43361    ./data/train_images/139067673.jpeg\n",
       "43362    ./data/train_images/195383621.jpeg\n",
       "Name: image_path, Length: 43363, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "target_names = pd.read_csv('./data/target_name_meta.tsv', sep='\\t')\n",
    "BASE_PATH = \"./data\"\n",
    "train_df['image_path'] = f'{BASE_PATH}/train_images/'+train_df['id'].astype(str)+'.jpeg'\n",
    "test_df['image_path'] = f'{BASE_PATH}/test_images/'+test_df['id'].astype(str)+'.jpeg'\n",
    "\n",
    "train_df['image_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ancillary data\n",
    "ancillary_columns = [col for col in train_df.columns if col.startswith(('WORLDCLIM_BIO', 'SOIL', 'MODIS', 'VOD'))]\n",
    "train_ancillary = train_df[ancillary_columns]\n",
    "test_ancillary = test_df[ancillary_columns]\n",
    "\n",
    "# Extract and transform targets\n",
    "target_columns = [col for col in train_df.columns if col.endswith('_mean')]\n",
    "train_targets = np.log1p(train_df[target_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43363, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Min-max normalize the transformed target data\n",
    "min_train = np.min(train_targets, axis=0)\n",
    "max_train = np.max(train_targets, axis=0)\n",
    "\n",
    "min_train_anc = np.min(train_ancillary, axis=0)\n",
    "max_train_anc = np.max(train_ancillary, axis=0)\n",
    "\n",
    "min_train_anc.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43363, 163)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ancillary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on same target vars, not log (maybe remove log and then normalize)\n",
    "train_targets_norm = (train_targets - min_train) / (max_train - min_train)\n",
    "train_ancillary_norm = (train_ancillary - min_train_anc) / (max_train_anc - min_train_anc)\n",
    "# train_targets_norm = np.expm1(train_targets_norm)\n",
    "train_targets_norm.shape\n",
    "train_ancillary_norm.shape\n",
    "\n",
    "test_ancillary_norm = (test_ancillary - min_train_anc) / (max_train_anc - min_train_anc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data for training and validation for ancillary model\n",
    "X_train_img, X_val_img, X_train_anc, X_val_anc, y_train, y_val = train_test_split(\n",
    "    pd.DataFrame(train_df['image_path'].values), train_ancillary_norm, train_targets_norm, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORLDCLIM_BIO1_annual_mean_temperature</th>\n",
       "      <th>WORLDCLIM_BIO12_annual_precipitation</th>\n",
       "      <th>WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month</th>\n",
       "      <th>WORLDCLIM_BIO15_precipitation_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO4_temperature_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO7_temperature_annual_range</th>\n",
       "      <th>SOIL_bdod_0.5cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_100.200cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_15.30cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_30.60cm_mean_0.01_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m03</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m04</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m05</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m06</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m07</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m08</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m09</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m10</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m11</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>0.718492</td>\n",
       "      <td>0.148879</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.091475</td>\n",
       "      <td>0.263473</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537743</td>\n",
       "      <td>0.498417</td>\n",
       "      <td>0.488927</td>\n",
       "      <td>0.466854</td>\n",
       "      <td>0.422058</td>\n",
       "      <td>0.434694</td>\n",
       "      <td>0.458408</td>\n",
       "      <td>0.467584</td>\n",
       "      <td>0.475004</td>\n",
       "      <td>0.484953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10646</th>\n",
       "      <td>0.315960</td>\n",
       "      <td>0.098020</td>\n",
       "      <td>0.031566</td>\n",
       "      <td>0.247534</td>\n",
       "      <td>0.358744</td>\n",
       "      <td>0.314560</td>\n",
       "      <td>0.585185</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701135</td>\n",
       "      <td>0.761288</td>\n",
       "      <td>0.827909</td>\n",
       "      <td>0.819837</td>\n",
       "      <td>0.754272</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.802987</td>\n",
       "      <td>0.806616</td>\n",
       "      <td>0.719488</td>\n",
       "      <td>0.693840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24766</th>\n",
       "      <td>0.432484</td>\n",
       "      <td>0.050598</td>\n",
       "      <td>0.017588</td>\n",
       "      <td>0.276269</td>\n",
       "      <td>0.366422</td>\n",
       "      <td>0.342553</td>\n",
       "      <td>0.570370</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522307</td>\n",
       "      <td>0.506097</td>\n",
       "      <td>0.546718</td>\n",
       "      <td>0.578595</td>\n",
       "      <td>0.551355</td>\n",
       "      <td>0.569474</td>\n",
       "      <td>0.566679</td>\n",
       "      <td>0.539655</td>\n",
       "      <td>0.511482</td>\n",
       "      <td>0.514989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20283</th>\n",
       "      <td>0.854769</td>\n",
       "      <td>0.267433</td>\n",
       "      <td>0.132266</td>\n",
       "      <td>0.413720</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.099332</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557999</td>\n",
       "      <td>0.534780</td>\n",
       "      <td>0.541611</td>\n",
       "      <td>0.533002</td>\n",
       "      <td>0.502840</td>\n",
       "      <td>0.510659</td>\n",
       "      <td>0.519714</td>\n",
       "      <td>0.516524</td>\n",
       "      <td>0.532241</td>\n",
       "      <td>0.554568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>0.771299</td>\n",
       "      <td>0.076534</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>0.543314</td>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.238038</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628553</td>\n",
       "      <td>0.600274</td>\n",
       "      <td>0.613608</td>\n",
       "      <td>0.627976</td>\n",
       "      <td>0.618468</td>\n",
       "      <td>0.645622</td>\n",
       "      <td>0.670352</td>\n",
       "      <td>0.676652</td>\n",
       "      <td>0.671136</td>\n",
       "      <td>0.666929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.662105</td>\n",
       "      <td>0.094222</td>\n",
       "      <td>0.039694</td>\n",
       "      <td>0.305918</td>\n",
       "      <td>0.161570</td>\n",
       "      <td>0.178936</td>\n",
       "      <td>0.585185</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.585185</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361392</td>\n",
       "      <td>0.326098</td>\n",
       "      <td>0.346955</td>\n",
       "      <td>0.325294</td>\n",
       "      <td>0.315932</td>\n",
       "      <td>0.319932</td>\n",
       "      <td>0.329771</td>\n",
       "      <td>0.340788</td>\n",
       "      <td>0.342192</td>\n",
       "      <td>0.345865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>0.903892</td>\n",
       "      <td>0.312920</td>\n",
       "      <td>0.145513</td>\n",
       "      <td>0.282260</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.048891</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454581</td>\n",
       "      <td>0.436537</td>\n",
       "      <td>0.446711</td>\n",
       "      <td>0.428182</td>\n",
       "      <td>0.399871</td>\n",
       "      <td>0.411214</td>\n",
       "      <td>0.422085</td>\n",
       "      <td>0.421952</td>\n",
       "      <td>0.435128</td>\n",
       "      <td>0.456732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>0.662223</td>\n",
       "      <td>0.088154</td>\n",
       "      <td>0.036556</td>\n",
       "      <td>0.305925</td>\n",
       "      <td>0.169441</td>\n",
       "      <td>0.193052</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367083</td>\n",
       "      <td>0.351960</td>\n",
       "      <td>0.353629</td>\n",
       "      <td>0.331461</td>\n",
       "      <td>0.318642</td>\n",
       "      <td>0.337477</td>\n",
       "      <td>0.354532</td>\n",
       "      <td>0.365566</td>\n",
       "      <td>0.368428</td>\n",
       "      <td>0.368840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.590818</td>\n",
       "      <td>0.123774</td>\n",
       "      <td>0.036316</td>\n",
       "      <td>0.195927</td>\n",
       "      <td>0.538892</td>\n",
       "      <td>0.569089</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.868056</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479448</td>\n",
       "      <td>0.504887</td>\n",
       "      <td>0.509196</td>\n",
       "      <td>0.552310</td>\n",
       "      <td>0.582199</td>\n",
       "      <td>0.611363</td>\n",
       "      <td>0.606360</td>\n",
       "      <td>0.558969</td>\n",
       "      <td>0.520983</td>\n",
       "      <td>0.526403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>0.700564</td>\n",
       "      <td>0.064412</td>\n",
       "      <td>0.055242</td>\n",
       "      <td>0.663343</td>\n",
       "      <td>0.323713</td>\n",
       "      <td>0.306509</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405577</td>\n",
       "      <td>0.382578</td>\n",
       "      <td>0.363797</td>\n",
       "      <td>0.342308</td>\n",
       "      <td>0.316309</td>\n",
       "      <td>0.321316</td>\n",
       "      <td>0.329931</td>\n",
       "      <td>0.328656</td>\n",
       "      <td>0.327197</td>\n",
       "      <td>0.348109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34690 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
       "16051                                0.718492   \n",
       "10646                                0.315960   \n",
       "24766                                0.432484   \n",
       "20283                                0.854769   \n",
       "8497                                 0.771299   \n",
       "...                                       ...   \n",
       "6265                                 0.662105   \n",
       "11284                                0.903892   \n",
       "38158                                0.662223   \n",
       "860                                  0.590818   \n",
       "15795                                0.700564   \n",
       "\n",
       "       WORLDCLIM_BIO12_annual_precipitation  \\\n",
       "16051                              0.148879   \n",
       "10646                              0.098020   \n",
       "24766                              0.050598   \n",
       "20283                              0.267433   \n",
       "8497                               0.076534   \n",
       "...                                     ...   \n",
       "6265                               0.094222   \n",
       "11284                              0.312920   \n",
       "38158                              0.088154   \n",
       "860                                0.123774   \n",
       "15795                              0.064412   \n",
       "\n",
       "       WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
       "16051                                           0.021161                       \n",
       "10646                                           0.031566                       \n",
       "24766                                           0.017588                       \n",
       "20283                                           0.132266                       \n",
       "8497                                            0.058094                       \n",
       "...                                                  ...                       \n",
       "6265                                            0.039694                       \n",
       "11284                                           0.145513                       \n",
       "38158                                           0.036556                       \n",
       "860                                             0.036316                       \n",
       "15795                                           0.055242                       \n",
       "\n",
       "       WORLDCLIM_BIO15_precipitation_seasonality  \\\n",
       "16051                                   0.091475   \n",
       "10646                                   0.247534   \n",
       "24766                                   0.276269   \n",
       "20283                                   0.413720   \n",
       "8497                                    0.543314   \n",
       "...                                          ...   \n",
       "6265                                    0.305918   \n",
       "11284                                   0.282260   \n",
       "38158                                   0.305925   \n",
       "860                                     0.195927   \n",
       "15795                                   0.663343   \n",
       "\n",
       "       WORLDCLIM_BIO4_temperature_seasonality  \\\n",
       "16051                                0.263473   \n",
       "10646                                0.358744   \n",
       "24766                                0.366422   \n",
       "20283                                0.035448   \n",
       "8497                                 0.072555   \n",
       "...                                       ...   \n",
       "6265                                 0.161570   \n",
       "11284                                0.024587   \n",
       "38158                                0.169441   \n",
       "860                                  0.538892   \n",
       "15795                                0.323713   \n",
       "\n",
       "       WORLDCLIM_BIO7_temperature_annual_range  SOIL_bdod_0.5cm_mean_0.01_deg  \\\n",
       "16051                                 0.311485                       0.748148   \n",
       "10646                                 0.314560                       0.585185   \n",
       "24766                                 0.342553                       0.570370   \n",
       "20283                                 0.099332                       0.629630   \n",
       "8497                                  0.238038                       0.696296   \n",
       "...                                        ...                            ...   \n",
       "6265                                  0.178936                       0.585185   \n",
       "11284                                 0.048891                       0.548148   \n",
       "38158                                 0.193052                       0.622222   \n",
       "860                                   0.569089                       0.785185   \n",
       "15795                                 0.306509                       0.822222   \n",
       "\n",
       "       SOIL_bdod_100.200cm_mean_0.01_deg  SOIL_bdod_15.30cm_mean_0.01_deg  \\\n",
       "16051                           0.736111                         0.740741   \n",
       "10646                           0.680556                         0.577778   \n",
       "24766                           0.680556                         0.600000   \n",
       "20283                           0.520833                         0.555556   \n",
       "8497                            0.569444                         0.629630   \n",
       "...                                  ...                              ...   \n",
       "6265                            0.645833                         0.585185   \n",
       "11284                           0.604167                         0.525926   \n",
       "38158                           0.652778                         0.622222   \n",
       "860                             0.868056                         0.837037   \n",
       "15795                           0.708333                         0.770370   \n",
       "\n",
       "       SOIL_bdod_30.60cm_mean_0.01_deg  ...  \\\n",
       "16051                         0.760870  ...   \n",
       "10646                         0.623188  ...   \n",
       "24766                         0.659420  ...   \n",
       "20283                         0.536232  ...   \n",
       "8497                          0.594203  ...   \n",
       "...                                ...  ...   \n",
       "6265                          0.623188  ...   \n",
       "11284                         0.550725  ...   \n",
       "38158                         0.644928  ...   \n",
       "860                           0.869565  ...   \n",
       "15795                         0.724638  ...   \n",
       "\n",
       "       VOD_X_1997_2018_multiyear_mean_m03  VOD_X_1997_2018_multiyear_mean_m04  \\\n",
       "16051                            0.537743                            0.498417   \n",
       "10646                            0.701135                            0.761288   \n",
       "24766                            0.522307                            0.506097   \n",
       "20283                            0.557999                            0.534780   \n",
       "8497                             0.628553                            0.600274   \n",
       "...                                   ...                                 ...   \n",
       "6265                             0.361392                            0.326098   \n",
       "11284                            0.454581                            0.436537   \n",
       "38158                            0.367083                            0.351960   \n",
       "860                              0.479448                            0.504887   \n",
       "15795                            0.405577                            0.382578   \n",
       "\n",
       "       VOD_X_1997_2018_multiyear_mean_m05  VOD_X_1997_2018_multiyear_mean_m06  \\\n",
       "16051                            0.488927                            0.466854   \n",
       "10646                            0.827909                            0.819837   \n",
       "24766                            0.546718                            0.578595   \n",
       "20283                            0.541611                            0.533002   \n",
       "8497                             0.613608                            0.627976   \n",
       "...                                   ...                                 ...   \n",
       "6265                             0.346955                            0.325294   \n",
       "11284                            0.446711                            0.428182   \n",
       "38158                            0.353629                            0.331461   \n",
       "860                              0.509196                            0.552310   \n",
       "15795                            0.363797                            0.342308   \n",
       "\n",
       "       VOD_X_1997_2018_multiyear_mean_m07  VOD_X_1997_2018_multiyear_mean_m08  \\\n",
       "16051                            0.422058                            0.434694   \n",
       "10646                            0.754272                            0.773536   \n",
       "24766                            0.551355                            0.569474   \n",
       "20283                            0.502840                            0.510659   \n",
       "8497                             0.618468                            0.645622   \n",
       "...                                   ...                                 ...   \n",
       "6265                             0.315932                            0.319932   \n",
       "11284                            0.399871                            0.411214   \n",
       "38158                            0.318642                            0.337477   \n",
       "860                              0.582199                            0.611363   \n",
       "15795                            0.316309                            0.321316   \n",
       "\n",
       "       VOD_X_1997_2018_multiyear_mean_m09  VOD_X_1997_2018_multiyear_mean_m10  \\\n",
       "16051                            0.458408                            0.467584   \n",
       "10646                            0.802987                            0.806616   \n",
       "24766                            0.566679                            0.539655   \n",
       "20283                            0.519714                            0.516524   \n",
       "8497                             0.670352                            0.676652   \n",
       "...                                   ...                                 ...   \n",
       "6265                             0.329771                            0.340788   \n",
       "11284                            0.422085                            0.421952   \n",
       "38158                            0.354532                            0.365566   \n",
       "860                              0.606360                            0.558969   \n",
       "15795                            0.329931                            0.328656   \n",
       "\n",
       "       VOD_X_1997_2018_multiyear_mean_m11  VOD_X_1997_2018_multiyear_mean_m12  \n",
       "16051                            0.475004                            0.484953  \n",
       "10646                            0.719488                            0.693840  \n",
       "24766                            0.511482                            0.514989  \n",
       "20283                            0.532241                            0.554568  \n",
       "8497                             0.671136                            0.666929  \n",
       "...                                   ...                                 ...  \n",
       "6265                             0.342192                            0.345865  \n",
       "11284                            0.435128                            0.456732  \n",
       "38158                            0.368428                            0.368840  \n",
       "860                              0.520983                            0.526403  \n",
       "15795                            0.327197                            0.348109  \n",
       "\n",
       "[34690 rows x 163 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_anc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['./data/train_images/75367331.jpeg'],\n",
       "       ['./data/train_images/192668447.jpeg'],\n",
       "       ['./data/train_images/195261292.jpeg'],\n",
       "       ...,\n",
       "       ['./data/train_images/184470019.jpeg'],\n",
       "       ['./data/train_images/196704567.jpeg'],\n",
       "       ['./data/train_images/194677586.jpeg']], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WORLDCLIM_BIO1_annual_mean_temperature                                   0.495795\n",
       "WORLDCLIM_BIO12_annual_precipitation                                     0.234190\n",
       "WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month    0.119638\n",
       "WORLDCLIM_BIO15_precipitation_seasonality                                0.364546\n",
       "WORLDCLIM_BIO4_temperature_seasonality                                   0.204074\n",
       "                                                                           ...   \n",
       "VOD_X_1997_2018_multiyear_mean_m08                                       0.469550\n",
       "VOD_X_1997_2018_multiyear_mean_m09                                       0.501000\n",
       "VOD_X_1997_2018_multiyear_mean_m10                                       0.530023\n",
       "VOD_X_1997_2018_multiyear_mean_m11                                       0.570208\n",
       "VOD_X_1997_2018_multiyear_mean_m12                                       0.618869\n",
       "Name: 41795, Length: 163, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_anc.iloc[16051]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WORLDCLIM_BIO1_annual_mean_temperature',\n",
       "       'WORLDCLIM_BIO12_annual_precipitation',\n",
       "       'WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month',\n",
       "       'WORLDCLIM_BIO15_precipitation_seasonality',\n",
       "       'WORLDCLIM_BIO4_temperature_seasonality',\n",
       "       'WORLDCLIM_BIO7_temperature_annual_range',\n",
       "       'SOIL_bdod_0.5cm_mean_0.01_deg', 'SOIL_bdod_100.200cm_mean_0.01_deg',\n",
       "       'SOIL_bdod_15.30cm_mean_0.01_deg', 'SOIL_bdod_30.60cm_mean_0.01_deg',\n",
       "       ...\n",
       "       'VOD_X_1997_2018_multiyear_mean_m03',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m04',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m05',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m06',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m07',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m08',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m09',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m10',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m11',\n",
       "       'VOD_X_1997_2018_multiyear_mean_m12'],\n",
       "      dtype='object', length=163)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_anc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260178"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X4_mean        0.256900\n",
       "X11_mean       4.904341\n",
       "X18_mean       9.888375\n",
       "X26_mean       8.149139\n",
       "X50_mean       2.681370\n",
       "X3112_mean    12.893096\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X4_mean</th>\n",
       "      <th>X11_mean</th>\n",
       "      <th>X18_mean</th>\n",
       "      <th>X26_mean</th>\n",
       "      <th>X50_mean</th>\n",
       "      <th>X3112_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.746281</td>\n",
       "      <td>0.237248</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>0.008633</td>\n",
       "      <td>0.652971</td>\n",
       "      <td>0.118959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.701309</td>\n",
       "      <td>0.524265</td>\n",
       "      <td>0.029574</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.259734</td>\n",
       "      <td>0.091455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.087936</td>\n",
       "      <td>0.138284</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.650511</td>\n",
       "      <td>0.003729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.535422</td>\n",
       "      <td>0.723667</td>\n",
       "      <td>0.144579</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>0.326607</td>\n",
       "      <td>0.315008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.721260</td>\n",
       "      <td>0.541546</td>\n",
       "      <td>0.090828</td>\n",
       "      <td>0.043050</td>\n",
       "      <td>0.424622</td>\n",
       "      <td>0.443002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43358</th>\n",
       "      <td>0.743541</td>\n",
       "      <td>0.378794</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.205110</td>\n",
       "      <td>0.045957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43359</th>\n",
       "      <td>0.916569</td>\n",
       "      <td>0.255141</td>\n",
       "      <td>0.734849</td>\n",
       "      <td>0.135033</td>\n",
       "      <td>0.458783</td>\n",
       "      <td>0.311850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>0.631141</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.389111</td>\n",
       "      <td>0.018660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43361</th>\n",
       "      <td>0.661591</td>\n",
       "      <td>0.511980</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.369059</td>\n",
       "      <td>0.006932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43362</th>\n",
       "      <td>0.586418</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.021648</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.357448</td>\n",
       "      <td>0.043460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43363 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X4_mean  X11_mean  X18_mean  X26_mean  X50_mean  X3112_mean\n",
       "0      0.746281  0.237248  0.038194  0.008633  0.652971    0.118959\n",
       "1      0.701309  0.524265  0.029574  0.005407  0.259734    0.091455\n",
       "2      0.998969  0.087936  0.138284  0.000112  0.650511    0.003729\n",
       "3      0.535422  0.723667  0.144579  0.031804  0.326607    0.315008\n",
       "4      0.721260  0.541546  0.090828  0.043050  0.424622    0.443002\n",
       "...         ...       ...       ...       ...       ...         ...\n",
       "43358  0.743541  0.378794  0.013057  0.000019  0.205110    0.045957\n",
       "43359  0.916569  0.255141  0.734849  0.135033  0.458783    0.311850\n",
       "43360  0.631141  0.513172  0.009935  0.000136  0.389111    0.018660\n",
       "43361  0.661591  0.511980  0.008155  0.000063  0.369059    0.006932\n",
       "43362  0.586418  0.414966  0.021648  0.006140  0.357448    0.043460\n",
       "\n",
       "[43363 rows x 6 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model want to use these features as input, but you need to normalize for input data\n",
    "# normalize to -1 and 1 by standard scaling it\n",
    "# drop log? and just use original values but normalized, drop 99.5% and less than 0.5% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>./data/train_images/75367331.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10646</th>\n",
       "      <td>./data/train_images/192668447.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24766</th>\n",
       "      <td>./data/train_images/195261292.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20283</th>\n",
       "      <td>./data/train_images/192546297.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>./data/train_images/133774591.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>./data/train_images/192047767.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>./data/train_images/140018028.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>./data/train_images/184470019.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>./data/train_images/196704567.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>./data/train_images/194677586.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34690 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0\n",
       "16051   ./data/train_images/75367331.jpeg\n",
       "10646  ./data/train_images/192668447.jpeg\n",
       "24766  ./data/train_images/195261292.jpeg\n",
       "20283  ./data/train_images/192546297.jpeg\n",
       "8497   ./data/train_images/133774591.jpeg\n",
       "...                                   ...\n",
       "6265   ./data/train_images/192047767.jpeg\n",
       "11284  ./data/train_images/140018028.jpeg\n",
       "38158  ./data/train_images/184470019.jpeg\n",
       "860    ./data/train_images/196704567.jpeg\n",
       "15795  ./data/train_images/194677586.jpeg\n",
       "\n",
       "[34690 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaimil.d/miniconda3/envs/econ424/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/var/folders/jn/v2jdn4p97ms6j6kbs3y53tsc0000gn/T/ipykernel_81091/1795983815.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ancillary_data = torch.tensor(self.ancillary_data.iloc[idx], dtype=torch.float)\n",
      "/var/folders/jn/v2jdn4p97ms6j6kbs3y53tsc0000gn/T/ipykernel_81091/1795983815.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  target = torch.tensor(self.targets.iloc[idx], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# Feature extractor for ViT\n",
    "feature_extractor = ViTFeatureExtractor(do_resize=True, size=224).from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Custom dataset class\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, image_paths, ancillary_data, targets=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.ancillary_data = ancillary_data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"trying to retrieve id: {idx}\")\n",
    "        # print(f\"can't find it: {self.image_paths[idx]}\")\n",
    "        image = plt.imread(str(self.image_paths[idx]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # print(f\"gets to anc data of length: {len(self.ancillary_data)}\")\n",
    "        # print(f\"{self.ancillary_data.iloc[idx]}\")\n",
    "        ancillary_data = torch.tensor(self.ancillary_data.iloc[idx], dtype=torch.float)\n",
    "        # print('gets to targets')\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets.iloc[idx], dtype=torch.float)\n",
    "            return image, ancillary_data, target\n",
    "        return image, ancillary_data\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(CFG.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "dataset = PlantDataset(train_df['image_path'], train_ancillary_norm, train_targets_norm, transform=transform)\n",
    "test_dataset = PlantDataset(test_df['image_path'], test_ancillary_norm, transform=transform)\n",
    "trainset, testset = train_test_split(dataset, test_size= 0.2)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(trainset, batch_size=CFG.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(testset, batch_size=CFG.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (ancillary_net): Sequential(\n",
       "    (0): Linear(in_features=163, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=832, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes, ancillary_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.ancillary_net = nn.Sequential(\n",
    "            nn.Linear(ancillary_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768 + 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, ancillary):\n",
    "        vit_features = self.vit(pixel_values=images).pooler_output\n",
    "        ancillary_features = self.ancillary_net(ancillary)\n",
    "        combined = torch.cat((vit_features, ancillary_features), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CombinedModel(num_classes=CFG.num_classes, ancillary_dim=X_train_anc.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "print(device)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Train XGBoost model on ancillary data\n",
    "xgb_model.fit(train_ancillary_norm, train_targets_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        ./data/train_images/101801795.jpeg\n",
      "1        ./data/train_images/115813315.jpeg\n",
      "2        ./data/train_images/173551949.jpeg\n",
      "3        ./data/train_images/148811120.jpeg\n",
      "4        ./data/train_images/195108876.jpeg\n",
      "                        ...                \n",
      "43358    ./data/train_images/172502909.jpeg\n",
      "43359    ./data/train_images/183294324.jpeg\n",
      "43360    ./data/train_images/108577580.jpeg\n",
      "43361    ./data/train_images/139067673.jpeg\n",
      "43362    ./data/train_images/195383621.jpeg\n",
      "Name: image_path, Length: 43363, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['image_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started epoch 0\n",
      "done 10 batches with loss: 0.24581088311970234\n",
      "done 20 batches with loss: 0.4786947909742594\n",
      "done 30 batches with loss: 0.716229947283864\n",
      "done 40 batches with loss: 0.9792259819805622\n",
      "done 50 batches with loss: 1.2260818034410477\n",
      "done 60 batches with loss: 1.4803067073225975\n",
      "done 70 batches with loss: 1.7240476571023464\n",
      "done 80 batches with loss: 1.9581753592938185\n",
      "done 90 batches with loss: 2.1982966791838408\n",
      "done 100 batches with loss: 2.4467715844511986\n",
      "done 110 batches with loss: 2.670454306527972\n",
      "done 120 batches with loss: 2.906502839177847\n",
      "done 130 batches with loss: 3.1484453342854977\n",
      "done 140 batches with loss: 3.3765039686113596\n",
      "done 150 batches with loss: 3.6160497032105923\n",
      "done 160 batches with loss: 3.838909083046019\n",
      "done 170 batches with loss: 4.037545613013208\n",
      "done 180 batches with loss: 4.265241087414324\n",
      "done 190 batches with loss: 4.497844177298248\n",
      "done 200 batches with loss: 4.727375137619674\n",
      "done 210 batches with loss: 4.937304326333106\n",
      "done 220 batches with loss: 5.1549519980326295\n",
      "done 230 batches with loss: 5.356672065332532\n",
      "done 240 batches with loss: 5.569557448849082\n",
      "done 250 batches with loss: 5.785587285645306\n",
      "done 260 batches with loss: 6.005039690993726\n",
      "done 270 batches with loss: 6.237251431681216\n",
      "done 280 batches with loss: 6.4499620562419295\n",
      "done 290 batches with loss: 6.663483559153974\n",
      "done 300 batches with loss: 6.8788089053705335\n",
      "done 310 batches with loss: 7.082300515845418\n",
      "done 320 batches with loss: 7.288741040974855\n",
      "done 330 batches with loss: 7.488230342045426\n",
      "done 340 batches with loss: 7.704636190086603\n",
      "done 350 batches with loss: 7.897595966234803\n",
      "done 360 batches with loss: 8.113780142739415\n",
      "done 370 batches with loss: 8.328331517986953\n",
      "done 380 batches with loss: 8.525150254368782\n",
      "done 390 batches with loss: 8.743103248998523\n",
      "done 400 batches with loss: 8.937021577730775\n",
      "done 410 batches with loss: 9.135452063754201\n",
      "done 420 batches with loss: 9.349126100540161\n",
      "done 430 batches with loss: 9.568848369643092\n",
      "done 440 batches with loss: 9.76404650695622\n",
      "done 450 batches with loss: 9.975736122578382\n",
      "done 460 batches with loss: 10.17229575663805\n",
      "done 470 batches with loss: 10.374718975275755\n",
      "done 480 batches with loss: 10.583445605821908\n",
      "done 490 batches with loss: 10.798875671811402\n",
      "done 500 batches with loss: 11.027761903591454\n",
      "done 510 batches with loss: 11.233949004672468\n",
      "done 520 batches with loss: 11.44464719016105\n",
      "done 530 batches with loss: 11.64220758434385\n",
      "done 540 batches with loss: 11.842007861472666\n",
      "done 550 batches with loss: 12.042958653531969\n",
      "done 560 batches with loss: 12.246697581373155\n",
      "done 570 batches with loss: 12.463587650097907\n",
      "done 580 batches with loss: 12.655978372320533\n",
      "done 590 batches with loss: 12.850537809543312\n",
      "done 600 batches with loss: 13.024943868629634\n",
      "done 610 batches with loss: 13.22939928341657\n",
      "done 620 batches with loss: 13.414198470301926\n",
      "done 630 batches with loss: 13.609893972985446\n",
      "done 640 batches with loss: 13.793173687532544\n",
      "done 650 batches with loss: 13.990848780609667\n",
      "done 660 batches with loss: 14.191474630497396\n",
      "done 670 batches with loss: 14.38525188434869\n",
      "done 680 batches with loss: 14.564237656071782\n",
      "done 690 batches with loss: 14.74931155424565\n",
      "done 700 batches with loss: 14.961056255735457\n",
      "done 710 batches with loss: 15.16923721600324\n",
      "done 720 batches with loss: 15.35272269602865\n",
      "done 730 batches with loss: 15.556463720276952\n",
      "done 740 batches with loss: 15.74300595279783\n",
      "done 750 batches with loss: 15.939028614200652\n",
      "done 760 batches with loss: 16.128626656718552\n",
      "done 770 batches with loss: 16.336027393117547\n",
      "done 780 batches with loss: 16.53710936382413\n",
      "done 790 batches with loss: 16.725626956671476\n",
      "done 800 batches with loss: 16.915207229554653\n",
      "done 810 batches with loss: 17.10366339609027\n",
      "done 820 batches with loss: 17.295986670069396\n",
      "done 830 batches with loss: 17.481729993596673\n",
      "done 840 batches with loss: 17.67117579933256\n",
      "done 850 batches with loss: 17.870365655981004\n",
      "done 860 batches with loss: 18.071132224984467\n",
      "done 870 batches with loss: 18.25522691849619\n",
      "done 880 batches with loss: 18.459031674079597\n",
      "done 890 batches with loss: 18.64429720956832\n",
      "done 900 batches with loss: 18.858733009546995\n",
      "done 910 batches with loss: 19.050237771123648\n",
      "done 920 batches with loss: 19.269989106804132\n",
      "done 930 batches with loss: 19.476789688691497\n",
      "done 940 batches with loss: 19.670875707641244\n",
      "done 950 batches with loss: 19.854165695607662\n",
      "done 960 batches with loss: 20.05505909770727\n",
      "done 970 batches with loss: 20.23134654853493\n",
      "done 980 batches with loss: 20.436983856372535\n",
      "done 990 batches with loss: 20.628940424881876\n",
      "done 1000 batches with loss: 20.828529860824347\n",
      "done 1010 batches with loss: 21.033383013680577\n",
      "done 1020 batches with loss: 21.230103560723364\n",
      "done 1030 batches with loss: 21.421548955142498\n",
      "done 1040 batches with loss: 21.623758477158844\n",
      "done 1050 batches with loss: 21.786447951570153\n",
      "done 1060 batches with loss: 21.96243247296661\n",
      "done 1070 batches with loss: 22.14600109681487\n",
      "done 1080 batches with loss: 22.312000951729715\n",
      "finished training...testing now.....\n",
      "Epoch 1/10, Train Loss: 0.02065819851830945, Val Loss: 0.016065257854814476\n",
      "Started epoch 1\n",
      "finished training...testing now.....\n",
      "Epoch 2/10, Train Loss: 0.01714484777541891, Val Loss: 0.015678233088264652\n",
      "Started epoch 2\n",
      "finished training...testing now.....\n",
      "Epoch 3/10, Train Loss: 0.014789810605467328, Val Loss: 0.01531785038733848\n",
      "Started epoch 3\n",
      "finished training...testing now.....\n",
      "Epoch 4/10, Train Loss: 0.012389520792213315, Val Loss: 0.015373704948133844\n",
      "Started epoch 4\n",
      "finished training...testing now.....\n",
      "Epoch 5/10, Train Loss: 0.010193720181399638, Val Loss: 0.015974745230350355\n",
      "Started epoch 5\n",
      "finished training...testing now.....\n",
      "Epoch 6/10, Train Loss: 0.008540693754630704, Val Loss: 0.017172749399940263\n",
      "Started epoch 6\n",
      "finished training...testing now.....\n",
      "Epoch 7/10, Train Loss: 0.007523137634225224, Val Loss: 0.015737729488618617\n",
      "Started epoch 7\n",
      "finished training...testing now.....\n",
      "Epoch 8/10, Train Loss: 0.006366284512206569, Val Loss: 0.01592206833212164\n",
      "Started epoch 8\n",
      "finished training...testing now.....\n",
      "Epoch 9/10, Train Loss: 0.005652821196826853, Val Loss: 0.016410305537112223\n",
      "Started epoch 9\n",
      "finished training...testing now.....\n",
      "Epoch 10/10, Train Loss: 0.00512904127875197, Val Loss: 0.01598092363736428\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(CFG.epochs):\n",
    "    print(f\"Started epoch {epoch}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    count = 1\n",
    "    for images, ancillary, targets in train_loader:\n",
    "        images, ancillary, targets = images.to(device), ancillary.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, ancillary)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if epoch < 1 and count % 10 == 0:\n",
    "            print(f\"done {count} batches with loss: {train_loss}\")    \n",
    "        count += 1\n",
    "    print(\"finished training...testing now.....\")\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, ancillary, targets in val_loader:\n",
    "            images, ancillary, targets = images.to(device), ancillary.to(device), targets.to(device)\n",
    "            outputs = model(images, ancillary)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CFG.epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/v2jdn4p97ms6j6kbs3y53tsc0000gn/T/ipykernel_81091/1795983815.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ancillary_data = torch.tensor(self.ancillary_data.iloc[idx], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate transformer model\n",
    "transformer_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, ancillary in test_loader:\n",
    "        images, ancillary = images.to(device), ancillary.to(device)\n",
    "        outputs = model(images, ancillary)\n",
    "        transformer_preds.append(outputs.cpu().numpy())\n",
    "transformer_preds = np.concatenate(transformer_preds, axis=0)\n",
    "\n",
    "xgb_preds = xgb_model.predict(test_ancillary_norm)\n",
    "\n",
    "# Combine predictions\n",
    "combined_preds = (transformer_preds + xgb_preds) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_transformers = [pred * (max_train - min_train) + min_train for pred in transformer_preds]\n",
    "test_predictions_xgb = [pred * (max_train - min_train) + min_train for pred in xgb_preds]\n",
    "test_predictions_xgb = np.expm1(test_predictions_xgb)\n",
    "# test_predictions_1 = (test_predictions_xgb + test_predictions_xgb)/2\n",
    "test_predictions = [pred * (max_train - min_train) + min_train for pred in combined_preds]\n",
    "test_predictions = np.expm1(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06485270e+00, 1.47848884e+02, 1.97073509e+04, 3.54538129e+03,\n",
       "        1.50895952e+01, 4.02236324e+05],\n",
       "       [1.05089537e+00, 1.48436786e+02, 1.97021496e+04, 3.48798406e+03,\n",
       "        1.52220723e+01, 3.99093673e+05],\n",
       "       [9.33579240e-01, 1.49649953e+02, 1.96993165e+04, 3.46116354e+03,\n",
       "        1.49233403e+01, 3.98142253e+05],\n",
       "       ...,\n",
       "       [1.13527719e+00, 1.44030916e+02, 1.97077393e+04, 3.52365345e+03,\n",
       "        1.52885503e+01, 4.00363069e+05],\n",
       "       [1.11396390e+00, 1.45240748e+02, 1.97066393e+04, 3.53477351e+03,\n",
       "        1.50473956e+01, 4.01071167e+05],\n",
       "       [9.80018360e-01, 1.48824433e+02, 1.96996062e+04, 3.46383393e+03,\n",
       "        1.50005250e+01, 3.98280493e+05]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission\n",
    "submission_df = pd.DataFrame(test_predictions_xgb, columns=target_names['trait_ID'])\n",
    "submission_df.insert(0, 'id', test_df['id'])\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ424",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
